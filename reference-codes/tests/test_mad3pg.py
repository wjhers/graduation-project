import pytest

import numpy as np

from tf2marl.agents import MAD3PGAgent
from tf2marl.common.util import FakeRun
from tf2marl.common.test_envs.identity_env import IdentityEnv  # , IdentityEnvBox
from tests.test_agent_env_combination import AgentEnvCombination
import tensorflow as tf

if tf.config.experimental.list_physical_devices('GPU'):
    tf.config.experimental.set_memory_growth(tf.config.experimental.list_physical_devices('GPU')[0],
                                             True)


def test_save_load():
    """
    Tests saving and loading for two agents.
    """
    fp = '/tmp/unittestmaddpg'
    env = IdentityEnv(5, 2)
    agents = [
        MAD3PGAgent(env.observation_space, env.action_space, idx, batch_size=32, buff_size=10000,
                    lr=0.01, num_layer=2,
                    num_units=32, gamma=0.9, tau=0.01, prioritized_replay=True,
                    max_step=5000) for idx in range(2)]

    for idx, agent in enumerate(agents):
        agent.save(fp + str(idx))

    load_agents = [
        MAD3PGAgent(env.observation_space, env.action_space, idx, batch_size=32, buff_size=10000,
                    lr=0.01, num_layer=2,
                    num_units=32, gamma=0.9, tau=0.01, prioritized_replay=True,
                    max_step=5000) for idx in range(2)]

    for idx, agent in enumerate(load_agents):
        agent.load(fp + str(idx))

    def check_for_equal_arrays(list_1, list_2):
        for el1, el2 in zip(list_1, list_2):
            if not (el1 == el2).all():
                return False
        return True

    assert check_for_equal_arrays(load_agents[0].critic.model.get_weights(),
                                  agents[0].critic.model.get_weights())
    assert check_for_equal_arrays(load_agents[1].critic.model.get_weights(),
                                  agents[1].critic.model.get_weights())
    assert check_for_equal_arrays(load_agents[0].critic_target.model.get_weights(),
                                  agents[0].critic_target.model.get_weights())
    assert check_for_equal_arrays(load_agents[1].critic_target.model.get_weights(),
                                  agents[1].critic_target.model.get_weights())

    assert check_for_equal_arrays(load_agents[0].policy.model.get_weights(),
                                  agents[0].policy.model.get_weights())
    assert check_for_equal_arrays(load_agents[1].policy.model.get_weights(),
                                  agents[1].policy.model.get_weights())
    assert check_for_equal_arrays(load_agents[0].policy_target.model.get_weights(),
                                  agents[0].policy_target.model.get_weights())
    assert check_for_equal_arrays(load_agents[1].policy_target.model.get_weights(),
                                  agents[1].policy_target.model.get_weights())


@pytest.mark.slow
@pytest.mark.parametrize("agent_number", [1, 2])
@pytest.mark.parametrize("prioritized_replay", [False])
# Note: prioritized replay does not seem to work well with Distributional DDPG and multiple
# agents.
def test_identity_discrete(agent_number, prioritized_replay):
    """
    Test if the algorithm (with a given policy)
    can learn an identity transformation (i.e. return observation as an action)
    :param model_name: (str) Name of the RL model
    """
    env = IdentityEnv(5, agent_number)

    lr = 0.01 if not prioritized_replay else 0.02
    agents = [
        MAD3PGAgent(env.observation_space, env.action_space, idx, batch_size=32, buff_size=10000,
                    lr=lr, num_layer=2,
                    num_units=32, gamma=0.9, tau=0.01, prioritized_replay=prioritized_replay,
                    max_step=50000, _run=FakeRun(), min_val=-25, max_val=0) for idx in
        range(agent_number)]

    ag_env_comb = AgentEnvCombination(agents, env)

    target_reward = -1.0 * agent_number
    episode_rewards = ag_env_comb.train(15000, 10, target_reward)  # 10000, 100)

    assert np.mean(episode_rewards[-10:]) > target_reward

# Todo: MAD3PG is currently not converging properly for continuous environments

# @pytest.mark.slow
# @pytest.mark.parametrize("agent_number", [1,2])
# @pytest.mark.parametrize("prioritized_replay", [False, True])
# def test_identity_continuous(agent_number, prioritized_replay):
#     """
#     Test if the algorithm (with a given policy)
#     can learn an identity transformation (i.e. return observation as an action), in a continuous environment.
#     :param model_name: (str) Name of the RL model
#     """
#
#     env = IdentityEnvBox(-1, 1, agent_number)
#     lr = 0.01 if not prioritized_replay else 0.02
#     agents = [MAD3PGAgent(env.observation_space, env.action_space, idx, batch_size=100, buff_size=10000, lr=lr, num_layer=2,
#                           num_units=32, gamma=0.9, tau=0.01, prioritized_replay=prioritized_replay,
#                           max_step=5000, _run=FakeRun()) for idx in range(agent_number)]
#
#     ag_env_comb = AgentEnvCombination(agents, env)
#
#     target_reward = -3.0 * agent_number
#
#     episode_rewards = ag_env_comb.train(6000, 10, target_reward)
#
#     assert np.mean(episode_rewards[-10:]) > target_reward
